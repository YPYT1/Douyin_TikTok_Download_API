"""
æŠ–éŸ³åˆé›†çˆ¬è™« - DrissionPageç‰ˆ v2ï¼ˆç½‘ç»œç›‘å¬æ–¹å¼è·å–è¯„è®ºï¼‰

ä¿®å¤é—®é¢˜ï¼š
1. ç›‘å¬å™¨çŠ¶æ€ç®¡ç†ä¼˜åŒ–
2. é…ç½®æ–‡ä»¶è·¯å¾„å¥å£®å¤„ç†
3. æ—¶é—´æˆ³å•ä½å…¼å®¹ï¼ˆç§’/æ¯«ç§’ï¼‰
4. ç™»å½•è¶…æ—¶é€»è¾‘ä¿®æ­£
5. APIå“åº”åŒ…ä¸¢å¤±é£é™©å¤„ç†
6. éªŒè¯ç æ£€æµ‹ä¸æš‚åœ
7. è¯¦ç»†è¿è¡Œæ—¥å¿—

æ—¥æœŸ: 2024å¹´
"""

import argparse
import csv
import json
import os
import re
import time
import datetime
import random
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from DrissionPage import ChromiumPage, ChromiumOptions


def log(msg: str, level: str = "INFO"):
    """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    prefix = {"INFO": "â„¹ï¸ ", "SUCCESS": "âœ…", "WARNING": "âš ï¸ ", "ERROR": "âŒ", "DEBUG": "ğŸ”", "PROGRESS": "ğŸ“Š"}.get(level, "  ")
    print(f"[{timestamp}] {prefix} {msg}")


def sanitize_filename(name: str, max_len: int = 60) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼Œå¹¶é™åˆ¶é•¿åº¦é¿å…Windowsè·¯å¾„è¿‡é•¿"""
    # ã€ä¼˜åŒ–4ã€‘ç¼©çŸ­æ–‡ä»¶åé•¿åº¦ï¼ŒWindowsè·¯å¾„æ€»é•¿åº¦é™åˆ¶260å­—ç¬¦
    clean_name = re.sub(r'[<>:"/\\|?*\n\r\t]', '_', name)
    return clean_name[:max_len]


def parse_timestamp(ts) -> str:
    """è§£ææ—¶é—´æˆ³ï¼Œå…¼å®¹ç§’å’Œæ¯«ç§’"""
    try:
        if ts is None or ts == 0:
            return ""
        ts = int(ts)
        if ts > 10000000000:  # å¤§äº10ä½æ˜¯æ¯«ç§’
            ts = ts // 1000
        return datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
    except:
        return str(ts)


class DrissionMixCrawler:
    """æŠ–éŸ³åˆé›†çˆ¬è™« - DrissionPageç‰ˆ v2"""
    
    def __init__(
        self,
        output_dir: Path = Path("output_drission"),
        max_comments: int = 2000,
        sleep: float = 3.0,
        headless: bool = False,
        login_wait: int = 60,
    ):
        self.output_dir = output_dir
        self.max_comments = max_comments
        self.sleep = sleep
        self.headless = headless
        self.login_wait = login_wait
        self.driver: Optional[ChromiumPage] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_videos': 0,
            'processed_videos': 0,
            'total_comments': 0,
            'success_videos': 0,
            'failed_videos': 0,
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        log("åˆå§‹åŒ–æµè§ˆå™¨...", "INFO")
        
        try:
            co = ChromiumOptions()
            if self.headless:
                co.headless(True)
                log("  ä½¿ç”¨æ— å¤´æ¨¡å¼", "DEBUG")
            
            self.driver = ChromiumPage(co)
            log("æµè§ˆå™¨å¯åŠ¨æˆåŠŸ", "SUCCESS")
            return True
        except Exception as e:
            log(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}", "ERROR")
            return False
    
    def _check_captcha(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å‡ºç°éªŒè¯ç å¼¹çª— - ä¼˜åŒ–ç‰ˆï¼Œå‡å°‘è¯¯æŠ¥"""
        try:
            # åªæ£€æµ‹ç‰¹å®šçš„éªŒè¯ç å¼¹çª—å…ƒç´ ï¼Œä¸æœç´¢æ•´ä¸ªé¡µé¢æ–‡æœ¬ï¼ˆé¿å…è¯¯æŠ¥ï¼‰
            # æŠ–éŸ³éªŒè¯ç é€šå¸¸æ˜¯ä¸€ä¸ªæ¨¡æ€å¼¹çª—
            captcha_selectors = [
                'xpath://div[contains(@class, "captcha-verify")]',
                'xpath://div[contains(@class, "secsdk-captcha")]',
                'xpath://div[contains(@class, "verify-wrap")]',
                'xpath://iframe[contains(@src, "captcha")]',
                'xpath://div[@id="captcha_container"]',
                'xpath://div[contains(@class, "slidetounlock")]',
            ]
            
            for selector in captcha_selectors:
                ele = self.driver.ele(selector, timeout=0.3)
                if ele:
                    return True
            
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«éªŒè¯ç å…³é”®è¯ï¼ˆè¿™ä¸ªæ¯”è¾ƒå¯é ï¼‰
            title = self.driver.title or ""
            if 'éªŒè¯' in title and ('ç ' in title or 'æ»‘å—' in title):
                return True
            
            return False
        except:
            return False
    
    def _handle_captcha(self):
        """å¤„ç†éªŒè¯ç """
        print()
        log("=" * 50, "WARNING")
        log("æ£€æµ‹åˆ°éªŒè¯ç /é£æ§ï¼ç¨‹åºæš‚åœ", "WARNING")
        log("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯", "WARNING")
        log("å®ŒæˆåæŒ‰å›è½¦é”®ç»§ç»­...", "WARNING")
        log("=" * 50, "WARNING")
        input()
        log("ç»§ç»­æ‰§è¡Œ...", "INFO")
    
    def _check_video_exists(self) -> bool:
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦å­˜åœ¨ï¼Œè¿”å› True è¡¨ç¤ºè§†é¢‘æœ‰æ•ˆ"""
        try:
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜
            title = self.driver.title or ""
            
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æ­£å¸¸çš„è§†é¢‘é¡µé¢ï¼ˆæŠ–éŸ³è§†é¢‘é¡µæ ‡é¢˜é€šå¸¸åŒ…å«ä½œè€…åæˆ–è§†é¢‘æ ‡é¢˜ï¼‰
            # å¦‚æœæ ‡é¢˜åŒ…å«æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯ï¼Œåˆ™è§†é¢‘ä¸å­˜åœ¨
            if 'ä¸å­˜åœ¨' in title or 'å·²åˆ é™¤' in title or '404' in title or 'é”™è¯¯' in title:
                return False
            
            # æ£€æŸ¥ URL æ˜¯å¦è¢«é‡å®šå‘åˆ°é”™è¯¯é¡µé¢
            current_url = self.driver.url or ""
            if '/error' in current_url or '/404' in current_url:
                return False
            
            # æ£€æŸ¥é¡µé¢å¯è§æ–‡æœ¬ä¸­æ˜¯å¦æœ‰æ˜ç¡®çš„"ä¸å­˜åœ¨"æç¤º
            # æ³¨æ„ï¼šåªæ£€æŸ¥ç‰¹å®šçš„é”™è¯¯æç¤ºå…ƒç´ ï¼Œé¿å…è¯¯åˆ¤
            error_texts = [
                'ä½œå“ä¸å­˜åœ¨',
                'è§†é¢‘ä¸å­˜åœ¨', 
                'å†…å®¹ä¸å­˜åœ¨',
                'è¯¥è§†é¢‘å·²åˆ é™¤',
                'è¯¥ä½œå“å·²åˆ é™¤',
                'é¡µé¢ä¸å­˜åœ¨',
                'æŠ±æ­‰ï¼Œé¡µé¢æœªæ‰¾åˆ°',
            ]
            
            # ä½¿ç”¨å…ƒç´ é€‰æ‹©å™¨æ£€æŸ¥é”™è¯¯æç¤ºï¼ˆæ›´ç²¾ç¡®ï¼‰
            error_selectors = [
                'xpath://div[contains(@class, "error")]//span',
                'xpath://div[contains(@class, "empty")]//p',
                'xpath://div[contains(@class, "videoNotFound")]',
                'xpath://div[contains(@class, "notFound")]',
            ]
            
            for selector in error_selectors:
                try:
                    ele = self.driver.ele(selector, timeout=0.5)
                    if ele:
                        ele_text = ele.text or ""
                        for error_text in error_texts:
                            if error_text in ele_text:
                                return False
                except:
                    pass
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘æ’­æ”¾å™¨å…ƒç´ ï¼ˆè§†é¢‘å­˜åœ¨çš„æ­£é¢è¯æ®ï¼‰
            video_selectors = [
                'xpath://video',
                'xpath://div[contains(@class, "xgplayer")]',
                'xpath://div[contains(@class, "video-player")]',
            ]
            
            for selector in video_selectors:
                try:
                    ele = self.driver.ele(selector, timeout=0.5)
                    if ele:
                        return True  # æ‰¾åˆ°è§†é¢‘æ’­æ”¾å™¨ï¼Œè§†é¢‘å­˜åœ¨
                except:
                    pass
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„é”™è¯¯æç¤ºï¼Œä¹Ÿæ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ’­æ”¾å™¨ï¼Œå¯èƒ½è¿˜åœ¨åŠ è½½ä¸­
            # é»˜è®¤è¿”å› Trueï¼Œè®©åç»­é€»è¾‘ç»§ç»­å¤„ç†
            return True
            
        except Exception as e:
            # æ£€æµ‹å‡ºé”™æ—¶é»˜è®¤è§†é¢‘å­˜åœ¨ï¼Œç»§ç»­å°è¯•
            return True
    
    def load_cookies(self) -> Optional[List[Tuple[str, str]]]:
        """ä» config.yaml åŠ è½½ Cookie - æ”¯æŒå¤šè·¯å¾„æŸ¥æ‰¾"""
        log("åŠ è½½ Cookie...", "INFO")
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®æ–‡ä»¶è·¯å¾„
        possible_paths = [
            Path("crawlers/douyin/web/config.yaml"),
            Path("config.yaml"),
            Path(__file__).parent / "crawlers/douyin/web/config.yaml",
        ]
        
        config_path = None
        for p in possible_paths:
            if p.exists():
                config_path = p
                log(f"  æ‰¾åˆ°é…ç½®æ–‡ä»¶: {p}", "DEBUG")
                break
        
        if not config_path:
            log("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æ‰«ç ç™»å½•", "WARNING")
            return None
        
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            cookie_str = config.get("TokenManager", {}).get("douyin", {}).get("headers", {}).get("Cookie", "")
            if not cookie_str:
                log("é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ Cookie", "WARNING")
                return None
            
            cookies = []
            for item in cookie_str.split(';'):
                item = item.strip()
                if '=' in item:
                    name, value = item.split('=', 1)
                    cookies.append((name.strip(), value.strip()))
            
            log(f"è§£æåˆ° {len(cookies)} ä¸ª Cookie", "SUCCESS")
            return cookies
            
        except ImportError:
            log("æœªå®‰è£… PyYAMLï¼Œè¯·è¿è¡Œ: pip install pyyaml", "ERROR")
            return None
        except Exception as e:
            log(f"åŠ è½½ Cookie å¤±è´¥: {e}", "ERROR")
            return None
    
    def check_login(self, cookies=None) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œæ”¯æŒæ‰«ç ç™»å½•"""
        log("è®¿é—®æŠ–éŸ³é¦–é¡µ...", "INFO")
        
        try:
            self.driver.get("https://www.douyin.com/")
            time.sleep(2)
            log("é¡µé¢åŠ è½½å®Œæˆ", "SUCCESS")
            
            # æ£€æŸ¥éªŒè¯ç 
            if self._check_captcha():
                self._handle_captcha()
            
            # è®¾ç½®Cookie
            if cookies:
                log(f"è®¾ç½® {len(cookies)} ä¸ª Cookie...", "DEBUG")
                success_count = 0
                for name, value in cookies:
                    try:
                        # ã€ä¼˜åŒ–5ã€‘å…ˆå°è¯•ä¸æŒ‡å®šdomainï¼Œè®©æµè§ˆå™¨è‡ªåŠ¨å¤„ç†
                        self.driver.set.cookies.set(name, value)
                        success_count += 1
                    except:
                        try:
                            # å¤‡ç”¨ï¼šæŒ‡å®šdomain
                            self.driver.set.cookies.set(name, value, domain='.douyin.com')
                            success_count += 1
                        except:
                            pass
                
                log(f"æˆåŠŸè®¾ç½® {success_count}/{len(cookies)} ä¸ª Cookie", "DEBUG")
                self.driver.refresh()
                time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç™»å½•
            if self._is_logged_in():
                log("Cookie æœ‰æ•ˆï¼Œå·²ç™»å½•çŠ¶æ€", "SUCCESS")
                return True
            
            log("æœªæ£€æµ‹åˆ°ç™»å½•çŠ¶æ€", "WARNING")
            
            if self.headless:
                log("æ— å¤´æ¨¡å¼æ— æ³•æ‰«ç ç™»å½•ï¼Œè¯·å…ˆé…ç½®æœ‰æ•ˆçš„ Cookie", "ERROR")
                return False
            
            return self._wait_for_qr_login()
            
        except Exception as e:
            log(f"æ£€æŸ¥ç™»å½•å¤±è´¥: {e}", "ERROR")
            return False
    
    def _is_logged_in(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        try:
            page_text = self.driver.html
            # æ£€æŸ¥ç™»å½•æ ‡è¯†
            if 'é€€å‡ºç™»å½•' in page_text or 'æ¶ˆæ¯' in page_text:
                return True
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤´åƒå…ƒç´ 
            avatar = self.driver.ele('xpath://img[contains(@class, "avatar")]', timeout=2)
            if avatar:
                return True
            return False
        except:
            return False
    
    def _wait_for_qr_login(self) -> bool:
        """ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½• - ä½¿ç”¨whileå¾ªç¯é¿å…é€’å½’æ ˆæº¢å‡º"""
        
        while True:  # ã€ä¼˜åŒ–3ã€‘ä½¿ç”¨whileå¾ªç¯æ›¿ä»£é€’å½’
            print()
            log("=" * 50, "INFO")
            log("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­æ‰«ç ç™»å½•æŠ–éŸ³", "INFO")
            log("   1. æ‰“å¼€æŠ–éŸ³APP", "INFO")
            log("   2. æ‰«ææµè§ˆå™¨ä¸­çš„äºŒç»´ç ", "INFO")
            log("   3. ç¡®è®¤ç™»å½•", "INFO")
            log("=" * 50, "INFO")
            log(f"ç­‰å¾…ç™»å½•ä¸­... (æœ€å¤šç­‰å¾… {self.login_wait} ç§’)", "INFO")
            
            # å°è¯•ç‚¹å‡»ç™»å½•æŒ‰é’®æ˜¾ç¤ºäºŒç»´ç 
            try:
                login_btn = self.driver.ele('xpath://button[contains(text(), "ç™»å½•")]', timeout=3)
                if login_btn:
                    login_btn.click()
                    time.sleep(1)
            except:
                pass
            
            start_time = time.time()
            
            while time.time() - start_time < self.login_wait:
                if self._is_logged_in():
                    log("ç™»å½•æˆåŠŸï¼", "SUCCESS")
                    self._save_cookies()
                    return True
                
                remaining = int(self.login_wait - (time.time() - start_time))
                print(f"\r   ç­‰å¾…ç™»å½•... å‰©ä½™ {remaining} ç§’   ", end='', flush=True)
                time.sleep(2)
            
            print()
            log("ç™»å½•è¶…æ—¶", "WARNING")
            
            # è¶…æ—¶åè¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
            response = input("æ˜¯å¦ç»§ç»­ç­‰å¾…ï¼Ÿ(y/n): ").strip().lower()
            if response != 'y':
                return False  # ç”¨æˆ·é€‰æ‹©ä¸ç»§ç»­ï¼Œè¿”å›False
            # å¦åˆ™ç»§ç»­å¤–å±‚whileå¾ªç¯ï¼Œé‡æ–°ç­‰å¾…
    
    def _save_cookies(self):
        """ä¿å­˜Cookieåˆ°æ–‡ä»¶"""
        try:
            cookies = self.driver.cookies()
            if cookies:
                cookie_file = self.output_dir / "cookies_saved.json"
                with open(cookie_file, 'w', encoding='utf-8') as f:
                    json.dump(cookies, f, ensure_ascii=False, indent=2)
                print(f"  ğŸ’¾ Cookieå·²ä¿å­˜åˆ°: {cookie_file}")
        except Exception as e:
            print(f"  ä¿å­˜Cookieå¤±è´¥: {e}")
    
    def get_mix_videos(self, mix_id: str) -> List[Dict]:
        """è·å–åˆé›†è§†é¢‘åˆ—è¡¨ - ä¼˜å…ˆä½¿ç”¨APIï¼Œå¤‡ç”¨é¡µé¢æ»šåŠ¨"""
        log(f"è·å–åˆé›†è§†é¢‘åˆ—è¡¨...", "INFO")
        
        videos = []
        actual_mix_id = mix_id
        
        try:
            # å¦‚æœæ˜¯é“¾æ¥ï¼Œå…ˆæå–mix_id
            if 'douyin.com' in mix_id or 'http' in mix_id:
                log(f"  æ£€æµ‹åˆ°é“¾æ¥ï¼Œæå–åˆé›†ID...", "DEBUG")
                self.driver.get(mix_id)
                time.sleep(4)
                current_url = self.driver.url
                log(f"  å½“å‰URL: {current_url}", "DEBUG")
                
                # ä»URLä¸­æå–mix_id
                match = re.search(r'modal_id=(\d+)|collection/(\d+)|mix_id=(\d+)', current_url)
                if match:
                    actual_mix_id = match.group(1) or match.group(2) or match.group(3)
                    log(f"  ä»URLæå–åˆ°åˆé›†ID: {actual_mix_id}", "SUCCESS")
                else:
                    # å¦‚æœé‡å®šå‘åˆ°è§†é¢‘é¡µé¢ï¼Œå°è¯•ä»é¡µé¢HTMLä¸­æå–åˆé›†ID
                    log(f"  URLä¸­æœªæ‰¾åˆ°åˆé›†IDï¼Œå°è¯•ä»é¡µé¢æå–...", "DEBUG")
                    page_html = self.driver.html or ""
                    
                    # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…
                    patterns = [
                        r'"mixId"\s*:\s*"(\d+)"',
                        r'"mix_id"\s*:\s*"(\d+)"',
                        r'collection/(\d+)',
                        r'modal_id=(\d+)',
                    ]
                    for pattern in patterns:
                        match = re.search(pattern, page_html)
                        if match:
                            actual_mix_id = match.group(1)
                            log(f"  ä»é¡µé¢æå–åˆ°åˆé›†ID: {actual_mix_id}", "SUCCESS")
                            break
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥é¡µé¢ä¸Šæ˜¯å¦æœ‰åˆé›†å…¥å£
                    if actual_mix_id == mix_id:
                        log(f"  å°è¯•ç‚¹å‡»åˆé›†å…¥å£...", "DEBUG")
                        try:
                            # æŸ¥æ‰¾åˆé›†ç›¸å…³çš„é“¾æ¥æˆ–æŒ‰é’®
                            mix_link = self.driver.ele('xpath://a[contains(@href, "collection") or contains(text(), "åˆé›†")]', timeout=3)
                            if mix_link:
                                mix_link.click()
                                time.sleep(3)
                                current_url = self.driver.url
                                match = re.search(r'collection/(\d+)|modal_id=(\d+)', current_url)
                                if match:
                                    actual_mix_id = match.group(1) or match.group(2)
                                    log(f"  ç‚¹å‡»åæå–åˆ°åˆé›†ID: {actual_mix_id}", "SUCCESS")
                        except:
                            pass
            
            # ç¡®ä¿actual_mix_idæ˜¯çº¯æ•°å­—
            if not actual_mix_id.isdigit():
                log(f"  æ— æ³•æå–æœ‰æ•ˆçš„åˆé›†IDï¼Œè¯·ç›´æ¥ä½¿ç”¨æ•°å­—ID", "ERROR")
                log(f"  æç¤ºï¼šè¿è¡Œ python crawl_mix_drission.py --mix-id 7326746646719498279", "INFO")
                return []
            
            # æ–¹æ³•1: ä½¿ç”¨ç°æœ‰APIæ¥å£è·å–ï¼ˆæ›´ç¨³å®šï¼‰
            log(f"  ä½¿ç”¨åˆé›†ID: {actual_mix_id}", "INFO")
            log(f"  å°è¯•ä½¿ç”¨APIè·å–è§†é¢‘åˆ—è¡¨...", "DEBUG")
            videos = self._fetch_mix_videos_api(actual_mix_id)
            
            if videos:
                log(f"  APIè·å–æˆåŠŸï¼", "SUCCESS")
            else:
                # æ–¹æ³•2: å¤‡ç”¨ - ä½¿ç”¨é¡µé¢æ»šåŠ¨æ–¹å¼
                log(f"  APIè·å–å¤±è´¥ï¼Œåˆ‡æ¢åˆ°é¡µé¢æ»šåŠ¨æ–¹å¼...", "WARNING")
                videos = self._fetch_mix_videos_scroll(actual_mix_id)
            
        except Exception as e:
            log(f"è·å–è§†é¢‘åˆ—è¡¨å¤±è´¥: {e}", "ERROR")
            import traceback
            traceback.print_exc()
        
        # é‡æ–°ç¼–å·
        for idx, v in enumerate(videos):
            v['index'] = idx + 1
        
        self.stats['total_videos'] = len(videos)
        log(f"å…±è·å– {len(videos)} ä¸ªè§†é¢‘", "SUCCESS")
        return videos
    
    def _fetch_mix_videos_api(self, mix_id: str) -> List[Dict]:
        """ä½¿ç”¨ç°æœ‰APIè·å–åˆé›†è§†é¢‘"""
        import asyncio
        from crawlers.douyin.web.web_crawler import DouyinWebCrawler
        
        videos = []
        
        async def fetch():
            nonlocal videos
            api_crawler = DouyinWebCrawler()
            cursor = 0
            page = 0
            
            while True:
                page += 1
                log(f"    APIè·å–ç¬¬ {page} é¡µ...", "DEBUG")
                
                try:
                    resp = await api_crawler.fetch_user_mix_videos(
                        mix_id=mix_id, cursor=cursor, count=20
                    )
                    
                    if not resp:
                        break
                    
                    aweme_list = resp.get("aweme_list") or []
                    if not aweme_list:
                        break
                    
                    for item in aweme_list:
                        video = {
                            'index': len(videos) + 1,
                            'aweme_id': item.get('aweme_id', ''),
                            'title': item.get('desc', ''),
                            'author': item.get('author', {}).get('nickname', ''),
                            'author_id': item.get('author', {}).get('sec_uid', ''),
                            'create_time': item.get('create_time', 0),
                            'duration': item.get('video', {}).get('duration', 0) // 1000,
                            'digg_count': item.get('statistics', {}).get('digg_count', 0),
                            'comment_count': item.get('statistics', {}).get('comment_count', 0),
                            'share_count': item.get('statistics', {}).get('share_count', 0),
                            'collect_count': item.get('statistics', {}).get('collect_count', 0),
                            'play_count': item.get('statistics', {}).get('play_count', 0),
                        }
                        videos.append(video)
                    
                    log(f"    å·²è·å– {len(videos)} ä¸ªè§†é¢‘", "PROGRESS")
                    
                    if not resp.get("has_more", False):
                        break
                    
                    cursor = resp.get("cursor", cursor + 20)
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    log(f"    APIè¯·æ±‚å¤±è´¥: {e}", "ERROR")
                    break
        
        try:
            asyncio.run(fetch())
        except Exception as e:
            log(f"  APIè°ƒç”¨å¼‚å¸¸: {e}", "ERROR")
        
        return videos
    
    def _fetch_mix_videos_scroll(self, mix_id: str) -> List[Dict]:
        """å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨é¡µé¢æ»šåŠ¨è·å–åˆé›†è§†é¢‘"""
        videos = []
        
        try:
            # å…ˆå¯åŠ¨ç›‘å¬ï¼Œå†è®¿é—®é¡µé¢
            self.driver.listen.start(['mix/aweme', 'mix/detail', 'aweme/v1/web/mix'])
            self.driver.listen.clear()
            
            mix_url = f"https://www.douyin.com/collection/{mix_id}"
            log(f"  è®¿é—®: {mix_url}", "DEBUG")
            self.driver.get(mix_url)
            time.sleep(4)
            
            if self._check_captcha():
                self._handle_captcha()
            
            last_count = 0
            no_new_count = 0
            scroll_round = 0
            
            log("  å¼€å§‹æ»šåŠ¨åŠ è½½è§†é¢‘åˆ—è¡¨...", "DEBUG")
            
            while no_new_count < 10:
                scroll_round += 1
                
                # æ£€æŸ¥éªŒè¯ç 
                if self._check_captcha():
                    self._handle_captcha()
                
                self.driver.scroll.down(500)
                time.sleep(1 + random.random())
                
                # ã€ä¿®å¤5ã€‘å¤„ç†å¤šä¸ªæ•°æ®åŒ…
                for _ in range(5):
                    resp = self.driver.listen.wait(timeout=0.5 if _ > 0 else 2)
                    if not resp:
                        break
                    
                    try:
                        json_data = resp.response.body
                        if json_data and 'aweme_list' in json_data:
                            aweme_list = json_data['aweme_list']
                            for item in aweme_list:
                                aweme_id = item.get('aweme_id', '')
                                if any(v['aweme_id'] == aweme_id for v in videos):
                                    continue
                                
                                video = {
                                    'index': len(videos) + 1,
                                    'aweme_id': aweme_id,
                                    'title': item.get('desc', ''),
                                    'author': item.get('author', {}).get('nickname', ''),
                                    'author_id': item.get('author', {}).get('sec_uid', ''),
                                    'create_time': item.get('create_time', 0),
                                    'duration': item.get('video', {}).get('duration', 0) // 1000,
                                    'digg_count': item.get('statistics', {}).get('digg_count', 0),
                                    'comment_count': item.get('statistics', {}).get('comment_count', 0),
                                    'share_count': item.get('statistics', {}).get('share_count', 0),
                                    'collect_count': item.get('statistics', {}).get('collect_count', 0),
                                    'play_count': item.get('statistics', {}).get('play_count', 0),
                                }
                                videos.append(video)
                            
                            if not json_data.get('has_more', True):
                                log(f"  å·²åŠ è½½å…¨éƒ¨è§†é¢‘", "DEBUG")
                                no_new_count = 100
                                break
                    except Exception as e:
                        log(f"  è§£æå“åº”å¤±è´¥: {e}", "DEBUG")
                
                if len(videos) > last_count:
                    no_new_count = 0
                    last_count = len(videos)
                    log(f"  ç¬¬ {scroll_round} è½®æ»šåŠ¨ï¼Œå·²è·å– {len(videos)} ä¸ªè§†é¢‘", "PROGRESS")
                else:
                    no_new_count += 1
            
            # åœæ­¢ç›‘å¬
            try:
                self.driver.listen.stop()
            except:
                pass
            
        except Exception as e:
            log(f"  æ»šåŠ¨è·å–å¤±è´¥: {e}", "ERROR")
        
        return videos
    
    def get_video_comments(self, aweme_id: str, expected_count: int = 0) -> Optional[List[Dict]]:
        """è·å–è§†é¢‘è¯„è®º - ä½¿ç”¨ç½‘ç»œç›‘å¬æ–¹å¼
        
        Returns:
            List[Dict]: è¯„è®ºåˆ—è¡¨
            None: è§†é¢‘ä¸å­˜åœ¨æˆ–å·²åˆ é™¤
        """
        comments = []
        comment_ids = set()
        
        log(f"  å¼€å§‹è·å–è¯„è®º (é¢„æœŸ: {expected_count} æ¡)...", "DEBUG")
        
        try:
            # ã€å…³é”®ä¿®å¤ã€‘åœæ­¢ä¹‹å‰çš„ç›‘å¬ï¼Œé‡æ–°å¯åŠ¨ï¼Œä½¿ç”¨æ›´é€šç”¨çš„URLåŒ¹é…
            try:
                self.driver.listen.stop()
            except:
                pass
            
            # ä½¿ç”¨è¯„è®ºåˆ—è¡¨æ¥å£ä½œä¸ºç›‘å¬ç›®æ ‡
            # åªç›‘å¬ä¸€çº§è¯„è®ºæ¥å£ï¼Œé¿å…æ··å…¥å›å¤æ¥å£å¯¼è‡´ has_more æå‰ç»“æŸ
            # ä¸€çº§è¯„è®º: /aweme/v1/web/comment/list/
            self.driver.listen.start('comment/list')
            self.driver.listen.clear()
            
            url = f"https://www.douyin.com/video/{aweme_id}"
            log(f"  è®¿é—®è§†é¢‘: {url}", "DEBUG")
            self.driver.get(url)
            time.sleep(4)  # å¢åŠ ç­‰å¾…æ—¶é—´
            
            # æ£€æŸ¥éªŒè¯ç 
            if self._check_captcha():
                self._handle_captcha()
            
            # æ£€æŸ¥è§†é¢‘æ˜¯å¦å­˜åœ¨ï¼ˆæ·»åŠ è¯Šæ–­ä¿¡æ¯ï¼‰
            if not self._check_video_exists():
                log(f"  âš ï¸ é¡µé¢æ˜¾ç¤ºè§†é¢‘ä¸å­˜åœ¨ï¼", "WARNING")
                log(f"  å½“å‰URL: {self.driver.url}", "DEBUG")
                log(f"  é¡µé¢æ ‡é¢˜: {self.driver.title}", "DEBUG")
                # æˆªå›¾ä¿å­˜è¯Šæ–­
                try:
                    screenshot_path = self.output_dir / f"debug_video_not_exist_{aweme_id}.png"
                    self.driver.get_screenshot(path=str(screenshot_path))
                    log(f"  å·²ä¿å­˜æˆªå›¾: {screenshot_path}", "DEBUG")
                except:
                    pass
                # å°è¯•åˆ·æ–°é¡µé¢é‡è¯•ä¸€æ¬¡
                log(f"  å°è¯•åˆ·æ–°é¡µé¢é‡è¯•...", "WARNING")
                self.driver.refresh()
                time.sleep(5)
                if not self._check_video_exists():
                    log(f"  åˆ·æ–°åä»ç„¶æ˜¾ç¤ºä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯è§†é¢‘è¢«åˆ é™¤æˆ–åœ°åŸŸé™åˆ¶", "ERROR")
                    return None
                else:
                    log(f"  åˆ·æ–°åé¡µé¢æ­£å¸¸ï¼Œç»§ç»­å¤„ç†", "SUCCESS")
            
            # ç­‰å¾…è¯„è®ºåŒºåŠ è½½
            log(f"  ç­‰å¾…è¯„è®ºåŒºåŠ è½½...", "DEBUG")
            time.sleep(2)
            
            # å…ˆå¤„ç†é¡µé¢åŠ è½½æ—¶å¯èƒ½å·²ç»äº§ç”Ÿçš„è¯„è®ºæ•°æ®åŒ…
            initial_count = 0
            for _ in range(15):
                resp = self.driver.listen.wait(timeout=0.5)
                if not resp:
                    break
                try:
                    json_data = resp.response.body
                    if json_data and 'comments' in json_data:
                        for comment in json_data['comments']:
                            comment_id = comment.get('cid', '') or str(comment.get('id', ''))
                            if comment_id and comment_id not in comment_ids:
                                comment_ids.add(comment_id)
                                parsed = self._parse_comment(comment)
                                comments.append(parsed)
                                initial_count += 1
                except:
                    pass
            
            if initial_count > 0:
                log(f"  åˆå§‹åŠ è½½è·å– {initial_count} æ¡è¯„è®º", "DEBUG")
            else:
                log(f"  åˆå§‹åŠ è½½æœªè·å–åˆ°è¯„è®ºï¼Œå°è¯•è§¦å‘åŠ è½½...", "WARNING")
                
                # æ–¹æ³•1ï¼šå°è¯•ç‚¹å‡»è¯„è®ºåŒº
                try:
                    comment_btn = self.driver.ele('xpath://span[contains(text(), "è¯„è®º") or contains(text(), "æ¡")]', timeout=2)
                    if comment_btn:
                        comment_btn.click()
                        time.sleep(2)
                        # å†æ¬¡å°è¯•è·å–
                        for _ in range(10):
                            resp = self.driver.listen.wait(timeout=0.5)
                            if not resp:
                                break
                            try:
                                json_data = resp.response.body
                                if json_data and 'comments' in json_data:
                                    for comment in json_data['comments']:
                                        comment_id = comment.get('cid', '') or str(comment.get('id', ''))
                                        if comment_id and comment_id not in comment_ids:
                                            comment_ids.add(comment_id)
                                            parsed = self._parse_comment(comment)
                                            comments.append(parsed)
                            except:
                                pass
                        if comments:
                            log(f"  ç‚¹å‡»åè·å– {len(comments)} æ¡è¯„è®º", "DEBUG")
                except:
                    pass
                
                # æ–¹æ³•2ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•åˆ·æ–°é¡µé¢
                if not comments:
                    log(f"  å°è¯•åˆ·æ–°é¡µé¢...", "WARNING")
                    self.driver.refresh()
                    time.sleep(4)
                    self.driver.listen.clear()
                    
                    for _ in range(10):
                        resp = self.driver.listen.wait(timeout=0.5)
                        if not resp:
                            break
                        try:
                            json_data = resp.response.body
                            if json_data and 'comments' in json_data:
                                for comment in json_data['comments']:
                                    comment_id = comment.get('cid', '') or str(comment.get('id', ''))
                                    if comment_id and comment_id not in comment_ids:
                                        comment_ids.add(comment_id)
                                        parsed = self._parse_comment(comment)
                                        comments.append(parsed)
                        except:
                            pass
                    if comments:
                        log(f"  åˆ·æ–°åè·å– {len(comments)} æ¡è¯„è®º", "DEBUG")
            
            # ã€å…³é”®ã€‘å…ˆæ»šåŠ¨åˆ°è¯„è®ºåŒºåŸŸ
            log(f"  æ»šåŠ¨åˆ°è¯„è®ºåŒºåŸŸ...", "DEBUG")
            
            # æ–¹æ³•1ï¼šç”¨ JS æ»šåŠ¨åˆ°é¡µé¢ä¸­éƒ¨ï¼ˆè§†é¢‘ä¸‹æ–¹å°±æ˜¯è¯„è®ºåŒºï¼‰
            try:
                # å…ˆæ»šåŠ¨åˆ°è§†é¢‘ä¸‹æ–¹
                self.driver.run_js('window.scrollTo(0, window.innerHeight * 0.8)')
                time.sleep(0.5)
                # å†å¾€ä¸‹æ»šåŠ¨ä¸€ç‚¹ç¡®ä¿è¯„è®ºåŒºå¯è§
                self.driver.run_js('window.scrollBy(0, 300)')
                time.sleep(0.5)
            except Exception as e:
                log(f"  JS æ»šåŠ¨å¤±è´¥: {e}ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•", "WARNING")
                try:
                    self.driver.scroll.to_half()
                    time.sleep(0.5)
                    self.driver.scroll.down(300)
                    time.sleep(0.5)
                except:
                    pass
            
            # æ£€æŸ¥è¯„è®ºåŒºæ˜¯å¦å¯è§ï¼ˆå°è¯•æ‰¾è¯„è®ºç›¸å…³å…ƒç´ ï¼‰
            try:
                comment_container = self.driver.ele('xpath://div[contains(@class, "comment")]', timeout=2)
                if comment_container:
                    log(f"  è¯„è®ºåŒºåŸŸå·²å®šä½", "DEBUG")
            except:
                log(f"  æœªæ‰¾åˆ°è¯„è®ºåŒºåŸŸå…ƒç´ ï¼Œç»§ç»­æ»šåŠ¨...", "DEBUG")
                self.driver.run_js('window.scrollBy(0, 500)')
                time.sleep(0.5)
            
            no_new_count = 0
            scroll_round = 0
            
            # æ ¹æ®é¢„æœŸè¯„è®ºæ•°è°ƒæ•´ç­–ç•¥
            is_large_comment = expected_count > 500
            max_scroll_rounds = max(300, expected_count // 3)
            max_no_new_rounds = 50 if is_large_comment else 30
            
            last_count = 0
            stall_rounds = 0  # è¯„è®ºæ•°åœæ»çš„è½®æ•°
            
            # æ»šåŠ¨è¾…åŠ©å‡½æ•°
            def get_scroll_position():
                """è·å–å½“å‰æ»šåŠ¨ä½ç½®"""
                try:
                    return self.driver.run_js('return window.pageYOffset || document.documentElement.scrollTop') or 0
                except:
                    return 0
            
            scroll_method = 0  # å½“å‰ä½¿ç”¨çš„æ»šåŠ¨æ–¹æ³•
            
            def js_scroll(distance):
                """å•æ¬¡æ»šåŠ¨ - å°è¯•å¤šç§æ–¹æ³•"""
                nonlocal scroll_method
                
                methods = [
                    # æ–¹æ³•0: window.scrollBy
                    lambda d: self.driver.run_js(f'window.scrollBy(0, {d})'),
                    # æ–¹æ³•1: document.documentElement.scrollTop
                    lambda d: self.driver.run_js(f'document.documentElement.scrollTop += {d}'),
                    # æ–¹æ³•2: document.body.scrollTop  
                    lambda d: self.driver.run_js(f'document.body.scrollTop += {d}'),
                    # æ–¹æ³•3: DrissionPage åŸç”Ÿæ»šåŠ¨
                    lambda d: self.driver.scroll.down(d),
                    # æ–¹æ³•4: æŒ‰ PageDown é”®
                    lambda d: self.driver.actions.key_down('PageDown').key_up('PageDown').perform(),
                ]
                
                # å…ˆå°è¯•å½“å‰æ–¹æ³•
                try:
                    methods[scroll_method](distance)
                    return True
                except:
                    pass
                
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                for i, method in enumerate(methods):
                    if i == scroll_method:
                        continue
                    try:
                        method(distance)
                        scroll_method = i  # åˆ‡æ¢åˆ°æœ‰æ•ˆçš„æ–¹æ³•
                        log(f"    åˆ‡æ¢åˆ°æ»šåŠ¨æ–¹æ³• {i}", "DEBUG")
                        return True
                    except:
                        continue
                
                return False
            
            # è®°å½•åˆå§‹æ»šåŠ¨ä½ç½®
            initial_scroll_pos = get_scroll_position()
            log(f"  å½“å‰æ»šåŠ¨ä½ç½®: {initial_scroll_pos}px", "DEBUG")
            
            # ä½¿ç”¨ Python å¾ªç¯æ»šåŠ¨ï¼ˆæ›´å¯é ï¼‰
            scroll_speed = 150 if is_large_comment else 100  # æ¯æ¬¡æ»šåŠ¨åƒç´ 
            log(f"  å¯åŠ¨æŒç»­æ»šåŠ¨ (æ¯æ¬¡ {scroll_speed}px)", "DEBUG")
            
            # å®æ—¶æ—¥å¿—å˜é‡
            last_log_time = time.time()
            last_log_count = 0
            start_time = time.time()
            
            last_scroll_pos = initial_scroll_pos
            scroll_failed_count = 0
            
            try:
                while scroll_round < max_scroll_rounds and len(comments) < self.max_comments:
                    scroll_round += 1
                    
                    # ã€å…³é”®ã€‘æ¯è½®éƒ½æ‰§è¡Œæ»šåŠ¨
                    js_scroll(scroll_speed)
                    time.sleep(0.15)  # æ»šåŠ¨åçŸ­æš‚ç­‰å¾…
                    
                    # æ¯10è½®æ£€æŸ¥æ»šåŠ¨æ˜¯å¦ç”Ÿæ•ˆ
                    if scroll_round % 10 == 0:
                        current_pos = get_scroll_position()
                        if current_pos <= last_scroll_pos + 10:  # å…è®¸10pxè¯¯å·®
                            scroll_failed_count += 1
                            if scroll_failed_count >= 2:
                                print()
                                log(f"    æ»šåŠ¨æœªç”Ÿæ•ˆ (ä½ç½®: {current_pos}px)ï¼Œåˆ‡æ¢æ»šåŠ¨æ–¹æ³•...", "WARNING")
                                # ç›´æ¥è°ƒç”¨ä¸åŒçš„æ»šåŠ¨æ–¹æ³•
                                try:
                                    self.driver.scroll.down(800)
                                except:
                                    try:
                                        self.driver.run_js('document.documentElement.scrollTop += 800')
                                    except:
                                        pass
                                # å°è¯•å¤§å¹…æ»šåŠ¨
                                js_scroll(800)
                                time.sleep(0.5)
                                scroll_failed_count = 0
                        else:
                            scroll_failed_count = 0
                        last_scroll_pos = current_pos
                    
                    # å¿«é€Ÿå¤„ç†æ•°æ®åŒ…
                    found_new = False
                    new_in_round = 0
                    for _ in range(10):
                        resp = self.driver.listen.wait(timeout=0.3)
                        if not resp:
                            break
                        
                        try:
                            json_data = resp.response.body
                            
                            if json_data and 'comments' in json_data:
                                new_comments = json_data['comments']
                                
                                for comment in new_comments:
                                    comment_id = comment.get('cid', '') or str(comment.get('id', ''))
                                    
                                    if comment_id and comment_id not in comment_ids:
                                        comment_ids.add(comment_id)
                                        parsed = self._parse_comment(comment)
                                        comments.append(parsed)
                                        found_new = True
                                        new_in_round += 1
                        except:
                            pass
                    
                    if found_new:
                        no_new_count = 0
                    else:
                        no_new_count += 1
                        # è¿ç»­å¤šæ¬¡æ²¡æ•°æ®æ—¶æ£€æŸ¥éªŒè¯ç 
                        if no_new_count >= 10:
                            if self._check_captcha():
                                self._handle_captcha()
                                no_new_count = 0
                                continue
                    
                    # ã€å®æ—¶æ—¥å¿—ã€‘æ¯2ç§’æˆ–æ¯è·å–20æ¡æ–°è¯„è®ºæ˜¾ç¤ºä¸€æ¬¡
                    current_time = time.time()
                    new_since_log = len(comments) - last_log_count
                    time_since_log = current_time - last_log_time
                    
                    if time_since_log >= 2 or new_since_log >= 20:
                        coverage = (len(comments) / expected_count * 100) if expected_count > 0 else 0
                        elapsed = current_time - start_time
                        speed = len(comments) / elapsed if elapsed > 0 else 0
                        level1 = sum(1 for c in comments if c.get('level', 1) == 1)
                        level2 = len(comments) - level1
                        
                        # ä½¿ç”¨ \r å®ç°åŒè¡Œåˆ·æ–°æ•ˆæœ
                        print(f"\r    ğŸ“Š å·²è·å– {len(comments):,} æ¡ (L1:{level1} L2:{level2}) | è¦†ç›–ç‡ {coverage:.1f}% | é€Ÿåº¦ {speed:.1f}æ¡/ç§’ | ç”¨æ—¶ {elapsed:.0f}ç§’    ", end='', flush=True)
                        
                        last_log_time = current_time
                        last_log_count = len(comments)
                        
                        # ã€ä¼˜åŒ–ã€‘è¦†ç›–ç‡è¾¾åˆ°100%æ—¶éªŒè¯å¹¶æå‰ç»“æŸ
                        if coverage >= 100:
                            # å†æ»šåŠ¨å‡ æ¬¡ç¡®è®¤çœŸçš„åˆ°åº•äº†
                            print()
                            log(f"    è¦†ç›–ç‡è¾¾åˆ° 100%ï¼ŒéªŒè¯ä¸­...", "DEBUG")
                            js_scroll(500)
                            time.sleep(0.5)
                            verify_found = False
                            for _ in range(5):
                                resp = self.driver.listen.wait(timeout=0.3)
                                if resp:
                                    try:
                                        json_data = resp.response.body
                                        if json_data and 'comments' in json_data:
                                            for comment in json_data['comments']:
                                                cid = comment.get('cid', '')
                                                if cid and cid not in comment_ids:
                                                    verify_found = True
                                                    comment_ids.add(cid)
                                                    comments.append(self._parse_comment(comment))
                                    except:
                                        pass
                            
                            if not verify_found:
                                log(f"    ç¡®è®¤å·²è·å–å…¨éƒ¨è¯„è®ºï¼Œæå‰ç»“æŸ", "SUCCESS")
                                break
                            else:
                                log(f"    è¿˜æœ‰æ›´å¤šè¯„è®ºï¼Œç»§ç»­æ»šåŠ¨...", "DEBUG")
                    
                    # æ¯10è½®æ£€æµ‹åœæ»
                    if scroll_round % 10 == 0:
                        if len(comments) == last_count:
                            stall_rounds += 1
                            # åœæ»æ—¶å°è¯•å¤§å¹…æ»šåŠ¨
                            if stall_rounds >= 2:
                                print()  # æ¢è¡Œ
                                log(f"    æ£€æµ‹åˆ°åœæ»ï¼Œå°è¯•å¤§å¹…æ»šåŠ¨...", "DEBUG")
                                js_scroll(1000)
                                time.sleep(0.5)
                        else:
                            stall_rounds = 0
                        last_count = len(comments)
                    
                    # æ¯30è½®å°è¯•å±•å¼€å›å¤
                    if scroll_round % 30 == 0:
                        self._try_expand_replies()
                    
                    # åº•éƒ¨æ£€æµ‹
                    if no_new_count >= max_no_new_rounds:
                        page_html = self.driver.html or ''
                        if 'æš‚æ—¶æ²¡æœ‰æ›´å¤šè¯„è®º' in page_html or 'æ²¡æœ‰æ›´å¤šäº†' in page_html:
                            print()
                            log(f"    æ£€æµ‹åˆ°è¯„è®ºåº•éƒ¨æ ‡è®°", "DEBUG")
                            break
                        
                        # å°è¯•æœ€åä¸€æ¬¡å¤§å¹…æ»šåŠ¨
                        js_scroll(2000)
                        time.sleep(0.8)
                        
                        # å†æ£€æŸ¥ä¸€æ¬¡
                        final_found = False
                        for _ in range(5):
                            resp = self.driver.listen.wait(timeout=0.3)
                            if resp:
                                try:
                                    json_data = resp.response.body
                                    if json_data and 'comments' in json_data:
                                        for comment in json_data['comments']:
                                            comment_id = comment.get('cid', '') or str(comment.get('id', ''))
                                            if comment_id and comment_id not in comment_ids:
                                                comment_ids.add(comment_id)
                                                parsed = self._parse_comment(comment)
                                                comments.append(parsed)
                                                final_found = True
                                except:
                                    pass
                        
                        if not final_found:
                            print()
                            log(f"    è¿ç»­ {no_new_count} æ¬¡æ— æ–°è¯„è®ºï¼Œç¡®è®¤åˆ°åº•", "DEBUG")
                            break
                        else:
                            no_new_count = 0
                    
                    # åœæ»å¤ªä¹…é€€å‡º
                    if stall_rounds >= 6:
                        log(f"    è¯„è®ºæ•°é•¿æ—¶é—´åœæ»ï¼Œç»“æŸ", "DEBUG")
                        break
                        
            except Exception as inner_e:
                print()
                log(f"    æ»šåŠ¨å¾ªç¯å¼‚å¸¸: {inner_e}", "WARNING")
            
            # æœ€ç»ˆç»Ÿè®¡
            print()  # æ¢è¡Œï¼ˆå› ä¸ºä¹‹å‰ç”¨äº† \rï¼‰
            elapsed_total = time.time() - start_time
            final_speed = len(comments) / elapsed_total if elapsed_total > 0 else 0
            level1_final = sum(1 for c in comments if c.get('level', 1) == 1)
            level2_final = len(comments) - level1_final
            print(f"    ğŸ“œ æ»šåŠ¨å®Œæˆ: {scroll_round} è½® | {len(comments)} æ¡è¯„è®º (L1:{level1_final} L2:{level2_final}) | å¹³å‡ {final_speed:.1f}æ¡/ç§’ | è€—æ—¶ {elapsed_total:.1f}ç§’")
            
        except Exception as e:
            print(f"    âœ— è·å–è¯„è®ºå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # ã€ä¿®å¤ã€‘ç¡®ä¿åœæ­¢ç›‘å¬å™¨ï¼Œé¿å…å½±å“ä¸‹ä¸€ä¸ªè§†é¢‘
            try:
                self.driver.listen.stop()
            except:
                pass
        
        return comments
    
    def _parse_comment(self, comment: Dict) -> Dict:
        """è§£æè¯„è®ºæ•°æ®ï¼ˆä»API JSONï¼‰"""
        try:
            # åŸºæœ¬ä¿¡æ¯
            comment_id = comment.get('cid', '') or str(comment.get('id', ''))
            text = comment.get('text', '').strip()
            create_time = comment.get('create_time', 0)
            digg_count = comment.get('digg_count', 0)
            reply_count = comment.get('reply_comment_total', 0)
            
            # ç”¨æˆ·ä¿¡æ¯
            user = comment.get('user', {})
            nickname = user.get('nickname', 'æœªçŸ¥ç”¨æˆ·')
            user_id = user.get('uid', '')
            sec_uid = user.get('sec_uid', '')
            
            # IPå±åœ°
            ip_label = comment.get('ip_label', '')
            
            # å›å¤ä¿¡æ¯
            reply_to_userid = comment.get('reply_to_userid', '') or ''
            reply_to_nickname = comment.get('reply_to_nickname', '') or ''
            
            # ã€ä¿®å¤3ã€‘æ—¶é—´æ ¼å¼åŒ–ï¼Œå…¼å®¹ç§’å’Œæ¯«ç§’
            time_str = parse_timestamp(create_time)
            
            # ã€ä¿®å¤ã€‘åˆ¤æ–­çº§åˆ« - æ›´ä¸¥æ ¼çš„åˆ¤æ–­
            # reply_id ä¸º "0" æˆ– 0 æˆ–ç©ºéƒ½è¡¨ç¤ºæ˜¯ä¸€çº§è¯„è®º
            reply_id = comment.get('reply_id', '')
            reply_id_str = str(reply_id) if reply_id else ''
            
            # åªæœ‰ reply_id æ˜¯éé›¶éç©ºçš„æœ‰æ•ˆIDï¼Œæˆ–è€…æœ‰æ˜ç¡®çš„ reply_to_userid æ—¶æ‰æ˜¯äºŒçº§è¯„è®º
            is_reply = False
            if reply_id_str and reply_id_str != '0' and reply_id_str != '':
                is_reply = True
            if reply_to_userid and str(reply_to_userid) != '0' and str(reply_to_userid) != '':
                is_reply = True
            
            level = 2 if is_reply else 1
            
            return {
                'cid': comment_id,
                'text': text,
                'user': nickname,
                'user_id': user_id,
                'user_sec_id': sec_uid,
                'digg_count': digg_count,
                'reply_count': reply_count,
                'create_time': time_str,
                'ip_location': ip_label,
                'level': level,
                'reply_to_user': reply_to_nickname,
                'reply_to_user_id': reply_to_userid,
            }
        except Exception as e:
            print(f"      è§£æè¯„è®ºå¤±è´¥: {e}")
            return {
                'cid': str(comment.get('cid', '')),
                'text': comment.get('text', ''),
                'user': 'æœªçŸ¥',
                'level': 1,
            }
    
    def _try_expand_replies(self):
        """å°è¯•å±•å¼€æ›´å¤šå›å¤"""
        try:
            # æŸ¥æ‰¾å±•å¼€æŒ‰é’®
            expand_btns = self.driver.eles('xpath://span[contains(text(), "å±•å¼€") or contains(text(), "æŸ¥çœ‹")]')
            for btn in expand_btns[:3]:
                try:
                    btn.click()
                    time.sleep(0.5)
                except:
                    pass
        except:
            pass
    
    def get_video_comments_api(self, aweme_id: str, expected_count: int = 0) -> List[Dict]:
        """ä½¿ç”¨ API è·å–è§†é¢‘ä¸€çº§è¯„è®ºï¼ˆé€‚ç”¨äºå¤§é‡è¯„è®ºçš„è§†é¢‘ï¼Œé€Ÿåº¦å¿«ï¼Œæ— éœ€æµè§ˆå™¨æ»šåŠ¨ï¼‰"""
        import asyncio
        
        comments = []
        comment_ids = set()
        
        log(f"  ä½¿ç”¨ API æ¨¡å¼è·å–ä¸€çº§è¯„è®ºï¼ˆæ— éœ€æ»šåŠ¨ï¼‰...", "INFO")
        
        try:
            # å¯¼å…¥ API çˆ¬è™«
            from crawlers.douyin.web.web_crawler import DouyinWebCrawler
            
            async def fetch_comments():
                crawler = DouyinWebCrawler()
                cursor = 0
                page = 0
                max_pages = 200  # æœ€å¤š200é¡µ
                
                while page < max_pages and len(comments) < self.max_comments:
                    page += 1
                    
                    try:
                        result = await crawler.fetch_video_comments(
                            aweme_id=aweme_id,
                            cursor=cursor,
                            count=50  # æ¯é¡µ50æ¡
                        )
                        
                        if not result:
                            log(f"    API ç¬¬ {page} é¡µè¿”å›ç©º", "WARNING")
                            break
                        
                        comments_data = result.get('comments', [])
                        if not comments_data:
                            log(f"    API ç¬¬ {page} é¡µæ— è¯„è®ºæ•°æ®", "DEBUG")
                            break
                        
                        for comment in comments_data:
                            comment_id = comment.get('cid', '')
                            if comment_id and comment_id not in comment_ids:
                                comment_ids.add(comment_id)
                                parsed = self._parse_comment(comment)
                                parsed['level'] = 1  # API è·å–çš„éƒ½æ˜¯ä¸€çº§è¯„è®º
                                comments.append(parsed)
                        
                        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤š
                        has_more = result.get('has_more', 0)
                        new_cursor = result.get('cursor', 0)
                        
                        # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                        coverage = (len(comments) / expected_count * 100) if expected_count > 0 else 0
                        print(f"\r    ğŸ“Š API ç¬¬ {page} é¡µ | å·²è·å– {len(comments):,} æ¡ä¸€çº§è¯„è®º | è¦†ç›–ç‡ {coverage:.1f}%    ", end='', flush=True)
                        
                        if not has_more or new_cursor == cursor:
                            print()  # æ¢è¡Œ
                            log(f"    API è·å–å®Œæˆï¼Œå…± {page} é¡µï¼Œ{len(comments)} æ¡è¯„è®º", "SUCCESS")
                            break
                        
                        cursor = new_cursor
                        await asyncio.sleep(0.3)  # API é—´éš”
                        
                    except Exception as e:
                        log(f"    API ç¬¬ {page} é¡µå¼‚å¸¸: {e}", "WARNING")
                        break
            
            # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
            asyncio.run(fetch_comments())
            
            if len(comments) > 0:
                log(f"  API æ¨¡å¼æˆåŠŸè·å– {len(comments)} æ¡è¯„è®º", "SUCCESS")
            else:
                log(f"  API æ¨¡å¼æœªè·å–åˆ°è¯„è®º", "WARNING")
            
        except ImportError as e:
            log(f"  æ— æ³•å¯¼å…¥ API æ¨¡å—: {e}ï¼Œè·³è¿‡è¯¥è§†é¢‘è¯„è®ºè·å–", "ERROR")
        except Exception as e:
            log(f"  API è·å–å¼‚å¸¸: {e}ï¼Œè·³è¿‡è¯¥è§†é¢‘è¯„è®ºè·å–", "ERROR")
        
        return comments
    
    def _reset_browser_state(self):
        """é‡ç½®æµè§ˆå™¨çŠ¶æ€ï¼Œç”¨äºè§†é¢‘é—´åˆ‡æ¢"""
        try:
            # åœæ­¢æ‰€æœ‰ç›‘å¬å™¨
            try:
                self.driver.listen.stop()
            except:
                pass
            
            # æ¸…é™¤ç›‘å¬å™¨ç¼“å­˜
            try:
                self.driver.listen.clear()
            except:
                pass
            
            # æ»šåŠ¨åˆ°é¡µé¢é¡¶éƒ¨
            try:
                self.driver.run_js('window.scrollTo(0, 0)')
            except:
                pass
                
        except Exception as e:
            pass  # å¿½ç•¥é‡ç½®è¿‡ç¨‹ä¸­çš„é”™è¯¯
    
    def process_video(self, video: Dict, crawl_comments: bool = True):
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        # ã€å…³é”®ã€‘åœ¨å¤„ç†æ–°è§†é¢‘å‰é‡ç½®æµè§ˆå™¨çŠ¶æ€
        self._reset_browser_state()
        
        idx = video['index']
        aweme_id = video['aweme_id']
        # ã€ä¿®å¤ã€‘æ˜¾ç¤ºåŸå§‹åˆé›†æ€»æ•°ï¼Œè€Œä¸æ˜¯å½“å‰æ‰¹æ¬¡æ€»æ•°
        total_original = self.stats.get('total_original', self.stats['total_videos'])
        
        folder = self.output_dir / f"{idx:03d}"
        folder.mkdir(exist_ok=True)
        
        title_short = (video['title'][:35] + '...') if len(video['title']) > 35 else video['title']
        
        print()
        log("=" * 60, "INFO")
        log(f"å¤„ç†è§†é¢‘ [{idx}/{total_original}]", "PROGRESS")
        log(f"  æ ‡é¢˜: {title_short}", "INFO")
        log(f"  ID: {aweme_id}", "INFO")
        log(f"  ä½œè€…: {video.get('author', 'æœªçŸ¥')}", "INFO")
        log(f"  æ—¶é•¿: {video.get('duration', 0)} ç§’", "INFO")
        log(f"  ç‚¹èµ: {video.get('digg_count', 0):,} | è¯„è®º: {video.get('comment_count', 0):,} | åˆ†äº«: {video.get('share_count', 0):,}", "INFO")
        log("=" * 60, "INFO")
        
        detail = {
            'aweme_id': aweme_id,
            'title': video.get('title', ''),
            'author': video.get('author', ''),
            'author_id': video.get('author_id', ''),
            'create_time': video.get('create_time', 0),
            'duration': video.get('duration', 0),
            'digg_count': video.get('digg_count', 0),
            'comment_count': video.get('comment_count', 0),
            'share_count': video.get('share_count', 0),
            'collect_count': video.get('collect_count', 0),
            'play_count': video.get('play_count', 0),
        }
        
        comments = []
        total_expected = video.get('comment_count', 0)
        
        # ã€ä¼˜åŒ–ã€‘è¯„è®ºæ•° > 1500 æ—¶ç”¨ API åªæŠ“ä¸€çº§è¯„è®ºï¼ˆæ›´å¿«ï¼‰ï¼Œå¦åˆ™ç”¨æµè§ˆå™¨æ»šåŠ¨æŠ“å…¨éƒ¨
        LARGE_COMMENT_THRESHOLD = 1500
        
        video_not_exist = False
        
        if crawl_comments and total_expected > 0:
            if total_expected > LARGE_COMMENT_THRESHOLD:
                log(f"  è¯„è®ºæ•° {total_expected} > {LARGE_COMMENT_THRESHOLD}ï¼Œä½¿ç”¨ API å¿«é€Ÿæ¨¡å¼ï¼ˆåªæŠ“ä¸€çº§è¯„è®ºï¼‰", "INFO")
                comments = self.get_video_comments_api(aweme_id, total_expected)
            else:
                comments = self.get_video_comments(aweme_id, total_expected)
            
            # æ£€æŸ¥è§†é¢‘æ˜¯å¦å­˜åœ¨ (get_video_comments è¿”å› None è¡¨ç¤ºè§†é¢‘ä¸å­˜åœ¨)
            if comments is None:
                video_not_exist = True
                comments = []
                log(f"è§†é¢‘ä¸å­˜åœ¨æˆ–å·²åˆ é™¤ï¼Œè·³è¿‡æ­¤è§†é¢‘", "WARNING")
            else:
                actual_count = len(comments)
                coverage = (actual_count / total_expected * 100) if total_expected > 0 else 0
                level1 = sum(1 for c in comments if c.get('level', 1) == 1)
                level2 = actual_count - level1
                
                self.stats['total_comments'] += actual_count
                
                if coverage >= 80:
                    log(f"è¯„è®ºè·å–å®Œæˆ: {actual_count}/{total_expected} æ¡ (è¦†ç›–ç‡ {coverage:.1f}%)", "SUCCESS")
                else:
                    log(f"è¯„è®ºè·å–å®Œæˆ: {actual_count}/{total_expected} æ¡ (è¦†ç›–ç‡ {coverage:.1f}%)", "WARNING")
                
                log(f"  ä¸€çº§è¯„è®º: {level1} æ¡ | äºŒçº§è¯„è®º: {level2} æ¡", "INFO")
        
        # å¦‚æœè§†é¢‘ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜
        if video_not_exist:
            self.stats['processed_videos'] += 1
            self.stats['failed_videos'] += 1
            return
        
        # ä¿å­˜CSV
        file_name = sanitize_filename(f"{video['title']}_{aweme_id}") + ".csv"
        file_path = folder / file_name
        self._save_csv(file_path, detail, comments)
        log(f"å·²ä¿å­˜: {file_name}", "SUCCESS")
        
        self.stats['processed_videos'] += 1
        self.stats['success_videos'] += 1
    
    def _save_csv(self, filepath: Path, video: Dict, comments: List[Dict]):
        """ä¿å­˜ä¸ºCSV"""
        fieldnames = [
            "åºå·", "è§†é¢‘ID", "è§†é¢‘æ ‡é¢˜", "è§†é¢‘URL", "å‘å¸ƒæ—¶é—´", "è§†é¢‘æ—¶é•¿(s)",
            "ä½œè€…æ˜µç§°", "ä½œè€…ID", "ç‚¹èµæ•°", "æ”¶è—æ•°", "åˆ†äº«æ•°", "æ’­æ”¾æ•°", "è¯„è®ºæ€»æ•°",
            "å±‚çº§", "è¯„è®ºID", "è¯„è®ºå†…å®¹", "è¯„è®ºç”¨æˆ·", "è¯„è®ºç”¨æˆ·ID",
            "è¯„è®ºç‚¹èµæ•°", "å›å¤æ•°", "è¯„è®ºæ—¶é—´", "IPå±åœ°", "å›å¤ç›®æ ‡ç”¨æˆ·"
        ]
        
        rows = []
        
        # è§†é¢‘ä¿¡æ¯è¡Œ
        video_row = {
            "åºå·": 1,
            "è§†é¢‘ID": video.get('aweme_id', ''),
            "è§†é¢‘æ ‡é¢˜": video.get('title', ''),
            "è§†é¢‘URL": f"https://www.douyin.com/video/{video.get('aweme_id', '')}",
            "å‘å¸ƒæ—¶é—´": video.get('create_time', ''),
            "è§†é¢‘æ—¶é•¿(s)": video.get('duration', ''),
            "ä½œè€…æ˜µç§°": video.get('author', ''),
            "ä½œè€…ID": video.get('author_id', ''),
            "ç‚¹èµæ•°": video.get('digg_count', 0),
            "æ”¶è—æ•°": video.get('collect_count', 0),
            "åˆ†äº«æ•°": video.get('share_count', 0),
            "æ’­æ”¾æ•°": video.get('play_count', 0),
            "è¯„è®ºæ€»æ•°": video.get('comment_count', 0),
            "å±‚çº§": "video",
        }
        rows.append(video_row)
        
        # è¯„è®ºè¡Œ
        for idx, c in enumerate(comments):
            comment_row = {
                "åºå·": idx + 2,
                "è§†é¢‘ID": video.get('aweme_id', ''),
                "å±‚çº§": f"L{c.get('level', 1)}",
                "è¯„è®ºID": c.get('cid', ''),
                "è¯„è®ºå†…å®¹": c.get('text', ''),
                "è¯„è®ºç”¨æˆ·": c.get('user', ''),
                "è¯„è®ºç”¨æˆ·ID": c.get('user_id', ''),
                "è¯„è®ºç‚¹èµæ•°": c.get('digg_count', 0),
                "å›å¤æ•°": c.get('reply_count', 0),
                "è¯„è®ºæ—¶é—´": c.get('create_time', ''),
                "IPå±åœ°": c.get('ip_location', ''),
                "å›å¤ç›®æ ‡ç”¨æˆ·": c.get('reply_to_user', ''),
            }
            rows.append(comment_row)
        
        with open(filepath, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for row in rows:
                writer.writerow({k: row.get(k, "") for k in fieldnames})
    
    def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
    
    def print_summary(self):
        """æ‰“å°çˆ¬å–æ€»ç»“"""
        print()
        log("=" * 60, "INFO")
        log("çˆ¬å–ä»»åŠ¡å®Œæˆ", "SUCCESS")
        log("=" * 60, "INFO")
        log(f"  æ€»è§†é¢‘æ•°:   {self.stats['total_videos']}", "INFO")
        log(f"  å·²å¤„ç†:     {self.stats['processed_videos']}", "INFO")
        log(f"  æˆåŠŸ:       {self.stats['success_videos']}", "SUCCESS")
        log(f"  å¤±è´¥:       {self.stats['failed_videos']}", "WARNING" if self.stats['failed_videos'] > 0 else "INFO")
        log(f"  æ€»è¯„è®ºæ•°:   {self.stats['total_comments']:,}", "INFO")
        log(f"  è¾“å‡ºç›®å½•:   {self.output_dir.resolve()}", "INFO")
        log("=" * 60, "INFO")
    
    def crawl_mix(self, mix_id: str, crawl_comments: bool = True, start: int = 0, end: int = 0):
        """çˆ¬å–å®Œæ•´åˆé›†
        
        Args:
            mix_id: åˆé›†IDæˆ–é“¾æ¥
            crawl_comments: æ˜¯å¦æŠ“å–è¯„è®º
            start: ä»ç¬¬å‡ ä¸ªå¼€å§‹ï¼ˆ1-basedï¼‰ï¼Œ0è¡¨ç¤ºäº¤äº’é€‰æ‹©
            end: åˆ°ç¬¬å‡ ä¸ªç»“æŸï¼ˆ1-basedï¼‰ï¼Œ0è¡¨ç¤ºåˆ°æœ€å
        """
        print()
        log("=" * 60, "INFO")
        log("DrissionPage æŠ–éŸ³åˆé›†çˆ¬è™« v2 (ç½‘ç»œç›‘å¬ç‰ˆ)", "INFO")
        log("=" * 60, "INFO")
        log(f"åˆé›†ID: {mix_id}", "INFO")
        log(f"è¾“å‡ºç›®å½•: {self.output_dir}", "INFO")
        log(f"æœ€å¤§è¯„è®ºæ•°: {self.max_comments}", "INFO")
        log(f"æŠ“å–è¯„è®º: {'æ˜¯' if crawl_comments else 'å¦'}", "INFO")
        log("=" * 60, "INFO")
        
        try:
            if not self.init_browser():
                return
            
            cookies = self.load_cookies()
            
            if not self.check_login(cookies):
                return
            
            videos = self.get_mix_videos(mix_id)
            if not videos:
                log("æœªè·å–åˆ°è§†é¢‘åˆ—è¡¨", "ERROR")
                return
            
            total_count = len(videos)
            log(f"å…±å‘ç° {total_count} ä¸ªè§†é¢‘", "SUCCESS")
            
            # ç¡®å®šçˆ¬å–åŒºé—´
            start_idx = start if start > 0 else 1
            end_idx = end if end > 0 else total_count
            
            # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†åŒºé—´ï¼Œç›´æ¥ä½¿ç”¨
            if start > 0 or end > 0:
                start_idx = max(1, min(start_idx, total_count))
                end_idx = max(start_idx, min(end_idx, total_count))
                log(f"å‘½ä»¤è¡ŒæŒ‡å®š: çˆ¬å–ç¬¬ {start_idx} åˆ°ç¬¬ {end_idx} ä¸ªè§†é¢‘ï¼ˆå…± {end_idx - start_idx + 1} ä¸ªï¼‰", "INFO")
            else:
                # äº¤äº’å¼è¯¢é—®åŒºé—´ï¼ˆéæ— å¤´æ¨¡å¼ä¸‹ï¼‰
                if not self.headless:
                    print()
                    log("è¯·é€‰æ‹©çˆ¬å–åŒºé—´ï¼ˆç›´æ¥å›è½¦è¡¨ç¤ºå…¨éƒ¨çˆ¬å–ï¼‰", "INFO")
                    
                    try:
                        start_input = input(f"  ä»ç¬¬å‡ ä¸ªè§†é¢‘å¼€å§‹? [1-{total_count}ï¼Œé»˜è®¤1]: ").strip()
                        if start_input:
                            start_idx = max(1, min(int(start_input), total_count))
                        
                        end_input = input(f"  åˆ°ç¬¬å‡ ä¸ªè§†é¢‘ç»“æŸ? [{start_idx}-{total_count}ï¼Œé»˜è®¤{total_count}]: ").strip()
                        if end_input:
                            end_idx = max(start_idx, min(int(end_input), total_count))
                    except ValueError:
                        log("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå…¨éƒ¨çˆ¬å–ï¼‰", "WARNING")
                        start_idx = 1
                        end_idx = total_count
                    except KeyboardInterrupt:
                        log("ç”¨æˆ·å–æ¶ˆ", "WARNING")
                        return
                
                print()
                if start_idx == 1 and end_idx == total_count:
                    log(f"å°†çˆ¬å–å…¨éƒ¨ {total_count} ä¸ªè§†é¢‘", "INFO")
                else:
                    log(f"å°†çˆ¬å–ç¬¬ {start_idx} åˆ°ç¬¬ {end_idx} ä¸ªè§†é¢‘ï¼ˆå…± {end_idx - start_idx + 1} ä¸ªï¼‰", "INFO")
            
            # æ ¹æ®åŒºé—´ç­›é€‰è§†é¢‘
            videos = videos[start_idx - 1:end_idx]
            
            # æ›´æ–°è§†é¢‘ç´¢å¼•ï¼ˆä¿æŒåŸå§‹ç¼–å·ï¼‰
            for i, video in enumerate(videos):
                video['index'] = start_idx + i
            
            # ã€ä¿®å¤ã€‘ä¿å­˜åŸå§‹åˆé›†æ€»æ•°ï¼Œç”¨äºæ˜¾ç¤ºæ­£ç¡®çš„åºå·
            self.stats['total_original'] = total_count
            self.stats['total_videos'] = len(videos)
            
            for video in videos:
                try:
                    self.process_video(video, crawl_comments)
                except Exception as e:
                    log(f"è§†é¢‘å¤„ç†å¼‚å¸¸: {e}", "ERROR")
                    self.stats['failed_videos'] += 1
                
                time.sleep(self.sleep)
            
            self.print_summary()
            
        finally:
            self.close_browser()


def main():
    parser = argparse.ArgumentParser(description="DrissionPage æŠ–éŸ³åˆé›†çˆ¬è™«ï¼ˆç½‘ç»œç›‘å¬ç‰ˆï¼‰")
    parser.add_argument("--mix-id", required=True, help="åˆé›†IDæˆ–åˆé›†é“¾æ¥(æ”¯æŒçŸ­é“¾æ¥)")
    parser.add_argument("--no-comments", action="store_true", help="ä¸æŠ“è¯„è®º")
    parser.add_argument("--max-comments", type=int, default=2000, help="å•è§†é¢‘æœ€å¤§è¯„è®ºæ•°")
    parser.add_argument("--start", type=int, default=0, help="ä»ç¬¬å‡ ä¸ªè§†é¢‘å¼€å§‹ï¼ˆ1-basedï¼‰ï¼Œ0è¡¨ç¤ºäº¤äº’é€‰æ‹©")
    parser.add_argument("--end", type=int, default=0, help="åˆ°ç¬¬å‡ ä¸ªè§†é¢‘ç»“æŸï¼ˆ1-basedï¼‰ï¼Œ0è¡¨ç¤ºåˆ°æœ€å")
    parser.add_argument("--sleep", type=float, default=3.0, help="è§†é¢‘é—´éš”ç§’æ•°")
    parser.add_argument("--out", type=str, default="output_drission", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--headless", action="store_true", help="æ— å¤´æ¨¡å¼")
    parser.add_argument("--login-wait", type=int, default=60, help="ç™»å½•ç­‰å¾…ç§’æ•°")
    args = parser.parse_args()
    
    crawler = DrissionMixCrawler(
        output_dir=Path(args.out),
        max_comments=args.max_comments,
        sleep=args.sleep,
        headless=args.headless,
        login_wait=args.login_wait,
    )
    
    crawler.crawl_mix(
        args.mix_id, 
        crawl_comments=not args.no_comments, 
        start=args.start,
        end=args.end
    )


if __name__ == "__main__":
    main()
