"""
æŠ–éŸ³åˆé›†çˆ¬è™« - çº¯APIç‰ˆæœ¬

ç›´æ¥è°ƒç”¨ crawlers/douyin/web/web_crawler.py ä¸­çš„æ¥å£ï¼š
1. åˆé›†è§†é¢‘åˆ—è¡¨ - fetch_user_mix_videos
2. ä¸€çº§è¯„è®º - fetch_video_comments  
3. äºŒçº§è¯„è®º - fetch_video_comments_reply

æ•°æ®æ ¼å¼ç¬¦åˆ tt.md è§„èŒƒ
æ— éœ€å¯åŠ¨ä»»ä½•HTTPæœåŠ¡ï¼Œç›´æ¥è¿è¡Œå³å¯ã€‚

æ—¥æœŸ: 2024å¹´
"""

import argparse
import asyncio
import csv
import json
import datetime
from pathlib import Path
from typing import Dict, List, Optional

# å¯¼å…¥æŠ–éŸ³APIçˆ¬è™«
from crawlers.douyin.web.web_crawler import DouyinWebCrawler


def log(msg: str, level: str = "INFO"):
    """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    icons = {
        "INFO": "â„¹ï¸ ",
        "SUCCESS": "âœ…",
        "WARNING": "âš ï¸ ",
        "ERROR": "âŒ",
        "DEBUG": "ğŸ”",
        "PROGRESS": "ğŸ“Š",
    }
    icon = icons.get(level, "")
    print(f"[{timestamp}] {icon} {msg}")


def sanitize_filename(name: str, max_length: int = 50) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    invalid_chars = '<>:"/\\|?*\n\r\t'
    for char in invalid_chars:
        name = name.replace(char, '')
    name = name.strip()
    if len(name) > max_length:
        name = name[:max_length]
    return name or "untitled"


class DouyinAPICrawler:
    """æŠ–éŸ³åˆé›†çˆ¬è™« - ç›´æ¥è°ƒç”¨Python API"""
    
    def __init__(self, 
                 output_dir: str = "output_api",
                 max_comments: int = 2000,
                 fetch_replies: bool = True):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            max_comments: æ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°
            fetch_replies: æ˜¯å¦è·å–äºŒçº§è¯„è®º
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.max_comments = max_comments
        self.fetch_replies = fetch_replies
        
        # APIçˆ¬è™«å®ä¾‹
        self.api = DouyinWebCrawler()
        
        # ç»Ÿè®¡
        self.stats = {
            'total_videos': 0,
            'processed_videos': 0,
            'total_comments': 0,
            'total_replies': 0,
            'failed_videos': 0,
        }
        
        # é…ç½®
        self.retry_times = 3
        self.retry_delay = 1
    
    async def fetch_mix_videos(self, mix_id: str) -> List[Dict]:
        """è·å–åˆé›†è§†é¢‘åˆ—è¡¨"""
        log(f"è·å–åˆé›†è§†é¢‘åˆ—è¡¨ (mix_id: {mix_id})...", "INFO")
        
        videos = []
        cursor = 0
        page = 0
        
        while True:
            page += 1
            log(f"  è·å–ç¬¬ {page} é¡µ...", "DEBUG")
            
            try:
                resp = await self.api.fetch_user_mix_videos(
                    mix_id=mix_id, cursor=cursor, count=20
                )
                
                if not resp:
                    log(f"  ç¬¬ {page} é¡µå“åº”ä¸ºç©º", "WARNING")
                    break
                
                aweme_list = resp.get('aweme_list', [])
                if not aweme_list:
                    log(f"  ç¬¬ {page} é¡µæ— è§†é¢‘æ•°æ®", "DEBUG")
                    break
                
                for item in aweme_list:
                    aweme_id = item.get('aweme_id', '')
                    if not aweme_id:
                        continue
                    
                    video = {
                        'index': len(videos) + 1,
                        'aweme_id': aweme_id,
                        'title': item.get('desc', ''),
                        'author': item.get('author', {}).get('nickname', ''),
                        'author_id': item.get('author', {}).get('sec_uid', ''),
                        'create_time': item.get('create_time', 0),
                        'duration': item.get('video', {}).get('duration', 0) // 1000,
                        'digg_count': item.get('statistics', {}).get('digg_count', 0),
                        'comment_count': item.get('statistics', {}).get('comment_count', 0),
                        'share_count': item.get('statistics', {}).get('share_count', 0),
                        'collect_count': item.get('statistics', {}).get('collect_count', 0),
                        'play_count': item.get('statistics', {}).get('play_count', 0),
                    }
                    videos.append(video)
                
                log(f"  å·²è·å– {len(videos)} ä¸ªè§†é¢‘", "PROGRESS")
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤š
                has_more = resp.get('has_more', False)
                if not has_more:
                    break
                
                new_cursor = resp.get('cursor', 0)
                if new_cursor == cursor:
                    break
                cursor = new_cursor
                
                await asyncio.sleep(0.5)
                
            except Exception as e:
                log(f"  è·å–ç¬¬ {page} é¡µå¤±è´¥: {e}", "ERROR")
                break
        
        log(f"å…±è·å– {len(videos)} ä¸ªè§†é¢‘", "SUCCESS")
        self.stats['total_videos'] = len(videos)
        return videos
    
    async def fetch_video_comments(self, aweme_id: str, expected_count: int = 0) -> List[Dict]:
        """è·å–è§†é¢‘è¯„è®ºï¼ˆä¸€çº§ + äºŒçº§ï¼‰"""
        log(f"  è·å–è¯„è®º (é¢„æœŸ: {expected_count})...", "DEBUG")
        
        comments = []
        comment_ids = set()
        cursor = 0
        
        while len(comments) < self.max_comments:
            try:
                resp = await self.api.fetch_video_comments(
                    aweme_id=aweme_id, cursor=cursor, count=50
                )
                
                if not resp:
                    break
                
                comment_list = resp.get('comments', [])
                if not comment_list:
                    break
                
                for comment in comment_list:
                    cid = comment.get('cid', '')
                    if not cid or cid in comment_ids:
                        continue
                    
                    comment_ids.add(cid)
                    
                    # è§£æä¸€çº§è¯„è®º
                    parsed = self._parse_comment(comment, level=1)
                    comments.append(parsed)
                    
                    # è·å–äºŒçº§è¯„è®º
                    reply_count = comment.get('reply_comment_total', 0)
                    if self.fetch_replies and reply_count > 0:
                        replies = await self.fetch_comment_replies(
                            aweme_id, cid, reply_count
                        )
                        for reply in replies:
                            if reply['comment_id'] not in comment_ids:
                                comment_ids.add(reply['comment_id'])
                                comments.append(reply)
                    
                    if len(comments) >= self.max_comments:
                        break
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤š
                has_more = resp.get('has_more', 0)
                new_cursor = resp.get('cursor', 0)
                
                if not has_more or new_cursor == cursor:
                    break
                
                cursor = new_cursor
                await asyncio.sleep(0.3)
                
            except Exception as e:
                log(f"  è·å–è¯„è®ºå¤±è´¥: {e}", "WARNING")
                break
        
        return comments
    
    async def fetch_comment_replies(self, aweme_id: str, comment_id: str, 
                                     expected_count: int = 0) -> List[Dict]:
        """è·å–è¯„è®ºçš„äºŒçº§å›å¤"""
        replies = []
        reply_ids = set()
        cursor = 0
        max_replies = min(expected_count, 100)  # æ¯æ¡è¯„è®ºæœ€å¤šè·å–100æ¡å›å¤
        
        while len(replies) < max_replies:
            try:
                resp = await self.api.fetch_video_comments_reply(
                    item_id=aweme_id,
                    comment_id=comment_id,
                    cursor=cursor,
                    count=20
                )
                
                if not resp:
                    break
                
                comment_list = resp.get('comments', [])
                if not comment_list:
                    break
                
                for reply in comment_list:
                    reply_id = reply.get('cid', '')
                    if not reply_id or reply_id in reply_ids:
                        continue
                    
                    reply_ids.add(reply_id)
                    parsed = self._parse_comment(reply, level=2)
                    replies.append(parsed)
                    
                    if len(replies) >= max_replies:
                        break
                
                has_more = resp.get('has_more', 0)
                new_cursor = resp.get('cursor', 0)
                
                if not has_more or new_cursor == cursor:
                    break
                
                cursor = new_cursor
                await asyncio.sleep(0.2)
                
            except Exception as e:
                log(f"  è·å–å›å¤å¤±è´¥: {e}", "WARNING")
                break
        
        self.stats['total_replies'] += len(replies)
        return replies
    
    def _parse_comment(self, comment: Dict, level: int = 1) -> Dict:
        """è§£æè¯„è®ºæ•°æ®"""
        try:
            comment_id = comment.get('cid', '')
            text = comment.get('text', '').strip()
            create_time = comment.get('create_time', 0)
            
            # æ—¶é—´å¤„ç†
            if create_time > 10000000000:
                create_time = create_time // 1000
            
            try:
                time_str = datetime.datetime.fromtimestamp(create_time).strftime('%Y-%m-%d %H:%M:%S')
            except:
                time_str = ''
            
            # ç”¨æˆ·ä¿¡æ¯
            user = comment.get('user', {})
            nickname = user.get('nickname', '')
            user_id = user.get('uid', '') or user.get('sec_uid', '')
            
            # IPå±åœ°
            ip_label = comment.get('ip_label', '')
            
            # å›å¤ç›®æ ‡
            reply_to = ''
            if comment.get('reply_to_userid'):
                reply_to = comment.get('reply_to_username', '') or str(comment.get('reply_to_userid', ''))
            
            return {
                'comment_id': comment_id,
                'text': text,
                'user': nickname,
                'user_id': user_id,
                'digg_count': comment.get('digg_count', 0),
                'reply_count': comment.get('reply_comment_total', 0),
                'create_time': time_str,
                'ip_label': ip_label,
                'level': level,
                'reply_to': reply_to,
            }
        except Exception as e:
            return {
                'comment_id': comment.get('cid', ''),
                'text': comment.get('text', ''),
                'user': '',
                'user_id': '',
                'digg_count': 0,
                'reply_count': 0,
                'create_time': '',
                'ip_label': '',
                'level': level,
                'reply_to': '',
            }
    
    async def process_video(self, video: Dict, crawl_comments: bool = True):
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        idx = video['index']
        aweme_id = video['aweme_id']
        total = self.stats['total_videos']
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        folder = self.output_dir / f"{idx:03d}"
        folder.mkdir(exist_ok=True)
        
        title_short = (video['title'][:35] + '...') if len(video['title']) > 35 else video['title']
        
        print()
        log("=" * 60, "INFO")
        log(f"å¤„ç†è§†é¢‘ [{idx}/{total}]", "PROGRESS")
        log(f"  æ ‡é¢˜: {title_short}", "INFO")
        log(f"  ID: {aweme_id}", "INFO")
        log(f"  ä½œè€…: {video.get('author', 'æœªçŸ¥')}", "INFO")
        log(f"  æ—¶é•¿: {video.get('duration', 0)} ç§’", "INFO")
        log(f"  ç‚¹èµ: {video.get('digg_count', 0):,} | è¯„è®º: {video.get('comment_count', 0):,} | åˆ†äº«: {video.get('share_count', 0):,}", "INFO")
        log("=" * 60, "INFO")
        
        # è§†é¢‘è¯¦æƒ…
        detail = {
            'aweme_id': aweme_id,
            'title': video.get('title', ''),
            'author': video.get('author', ''),
            'author_id': video.get('author_id', ''),
            'create_time': video.get('create_time', 0),
            'duration': video.get('duration', 0),
            'digg_count': video.get('digg_count', 0),
            'comment_count': video.get('comment_count', 0),
            'share_count': video.get('share_count', 0),
            'collect_count': video.get('collect_count', 0),
            'play_count': video.get('play_count', 0),
        }
        
        # è·å–è¯„è®º
        comments = []
        total_expected = video.get('comment_count', 0)
        
        if crawl_comments and total_expected > 0:
            comments = await self.fetch_video_comments(aweme_id, total_expected)
            
            actual_count = len(comments)
            coverage = (actual_count / total_expected * 100) if total_expected > 0 else 0
            level1 = sum(1 for c in comments if c.get('level', 1) == 1)
            level2 = actual_count - level1
            
            self.stats['total_comments'] += actual_count
            
            if coverage >= 50:
                log(f"è¯„è®ºè·å–å®Œæˆ: {actual_count}/{total_expected} æ¡ (è¦†ç›–ç‡ {coverage:.1f}%)", "SUCCESS")
            else:
                log(f"è¯„è®ºè·å–å®Œæˆ: {actual_count}/{total_expected} æ¡ (è¦†ç›–ç‡ {coverage:.1f}%)", "WARNING")
            
            log(f"  ä¸€çº§è¯„è®º: {level1} æ¡ | äºŒçº§è¯„è®º: {level2} æ¡", "INFO")
        
        # ä¿å­˜CSV
        file_name = sanitize_filename(f"{video['title']}_{aweme_id}") + ".csv"
        file_path = folder / file_name
        self._save_csv(file_path, detail, comments)
        log(f"å·²ä¿å­˜: {file_name}", "SUCCESS")
        
        # ä¿å­˜JSON
        json_path = folder / f"{aweme_id}.json"
        self._save_json(json_path, detail, comments)
        
        self.stats['processed_videos'] += 1
    
    def _save_csv(self, filepath: Path, video: Dict, comments: List[Dict]):
        """ä¿å­˜ä¸ºCSV - ç¬¦åˆtt.mdæ ¼å¼"""
        fieldnames = [
            "åºå·", "è§†é¢‘ID", "è§†é¢‘æ ‡é¢˜", "è§†é¢‘URL", "å‘å¸ƒæ—¶é—´", "è§†é¢‘æ—¶é•¿(s)",
            "ä½œè€…æ˜µç§°", "ä½œè€…ID", "ç‚¹èµæ•°", "æ”¶è—æ•°", "åˆ†äº«æ•°", "æ’­æ”¾æ•°", "è¯„è®ºæ€»æ•°",
            "å±‚çº§", "è¯„è®ºID", "è¯„è®ºå†…å®¹", "è¯„è®ºç”¨æˆ·", "è¯„è®ºç”¨æˆ·ID",
            "è¯„è®ºç‚¹èµæ•°", "å›å¤æ•°", "è¯„è®ºæ—¶é—´", "IPå±åœ°", "å›å¤ç›®æ ‡ç”¨æˆ·"
        ]
        
        rows = []
        
        # è§†é¢‘ä¿¡æ¯è¡Œ
        create_time = video.get('create_time', 0)
        if create_time > 10000000000:
            create_time = create_time // 1000
        
        video_row = {
            "åºå·": 1,
            "è§†é¢‘ID": video.get('aweme_id', ''),
            "è§†é¢‘æ ‡é¢˜": video.get('title', ''),
            "è§†é¢‘URL": f"https://www.douyin.com/video/{video.get('aweme_id', '')}",
            "å‘å¸ƒæ—¶é—´": create_time,
            "è§†é¢‘æ—¶é•¿(s)": video.get('duration', 0),
            "ä½œè€…æ˜µç§°": video.get('author', ''),
            "ä½œè€…ID": video.get('author_id', ''),
            "ç‚¹èµæ•°": video.get('digg_count', 0),
            "æ”¶è—æ•°": video.get('collect_count', 0),
            "åˆ†äº«æ•°": video.get('share_count', 0),
            "æ’­æ”¾æ•°": video.get('play_count', 0),
            "è¯„è®ºæ€»æ•°": video.get('comment_count', 0),
            "å±‚çº§": "video",
            "è¯„è®ºID": "",
            "è¯„è®ºå†…å®¹": "",
            "è¯„è®ºç”¨æˆ·": "",
            "è¯„è®ºç”¨æˆ·ID": "",
            "è¯„è®ºç‚¹èµæ•°": "",
            "å›å¤æ•°": "",
            "è¯„è®ºæ—¶é—´": "",
            "IPå±åœ°": "",
            "å›å¤ç›®æ ‡ç”¨æˆ·": "",
        }
        rows.append(video_row)
        
        # è¯„è®ºè¡Œ
        for idx, comment in enumerate(comments, start=2):
            level = comment.get('level', 1)
            level_str = f"L{level}"
            
            comment_row = {
                "åºå·": idx,
                "è§†é¢‘ID": video.get('aweme_id', ''),
                "è§†é¢‘æ ‡é¢˜": "",
                "è§†é¢‘URL": "",
                "å‘å¸ƒæ—¶é—´": "",
                "è§†é¢‘æ—¶é•¿(s)": "",
                "ä½œè€…æ˜µç§°": "",
                "ä½œè€…ID": "",
                "ç‚¹èµæ•°": "",
                "æ”¶è—æ•°": "",
                "åˆ†äº«æ•°": "",
                "æ’­æ”¾æ•°": "",
                "è¯„è®ºæ€»æ•°": "",
                "å±‚çº§": level_str,
                "è¯„è®ºID": comment.get('comment_id', ''),
                "è¯„è®ºå†…å®¹": comment.get('text', ''),
                "è¯„è®ºç”¨æˆ·": comment.get('user', ''),
                "è¯„è®ºç”¨æˆ·ID": comment.get('user_id', ''),
                "è¯„è®ºç‚¹èµæ•°": comment.get('digg_count', 0),
                "å›å¤æ•°": comment.get('reply_count', 0),
                "è¯„è®ºæ—¶é—´": comment.get('create_time', ''),
                "IPå±åœ°": comment.get('ip_label', ''),
                "å›å¤ç›®æ ‡ç”¨æˆ·": comment.get('reply_to', ''),
            }
            rows.append(comment_row)
        
        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)
    
    def _save_json(self, filepath: Path, video: Dict, comments: List[Dict]):
        """ä¿å­˜ä¸ºJSON"""
        data = {
            'video': video,
            'comments': comments,
            'stats': {
                'total_comments': len(comments),
                'level1_comments': sum(1 for c in comments if c.get('level', 1) == 1),
                'level2_comments': sum(1 for c in comments if c.get('level', 1) == 2),
            }
        }
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    async def crawl_mix(self, mix_id: str, crawl_comments: bool = True, limit: int = 0):
        """çˆ¬å–åˆé›†"""
        print()
        log("=" * 60, "INFO")
        log("æŠ–éŸ³åˆé›†çˆ¬è™« - çº¯APIç‰ˆæœ¬", "INFO")
        log("=" * 60, "INFO")
        log(f"åˆé›†ID: {mix_id}", "INFO")
        log(f"è¾“å‡ºç›®å½•: {self.output_dir}", "INFO")
        log(f"æœ€å¤§è¯„è®ºæ•°: {self.max_comments}", "INFO")
        log(f"æŠ“å–è¯„è®º: {'æ˜¯' if crawl_comments else 'å¦'}", "INFO")
        log(f"æŠ“å–äºŒçº§è¯„è®º: {'æ˜¯' if self.fetch_replies else 'å¦'}", "INFO")
        if limit > 0:
            log(f"é™åˆ¶è§†é¢‘æ•°: {limit}", "INFO")
        log("=" * 60, "INFO")
        
        # è·å–è§†é¢‘åˆ—è¡¨
        videos = await self.fetch_mix_videos(mix_id)
        if not videos:
            log("æœªè·å–åˆ°è§†é¢‘åˆ—è¡¨", "ERROR")
            return
        
        # é™åˆ¶è§†é¢‘æ•°é‡
        if limit > 0:
            videos = videos[:limit]
            self.stats['total_videos'] = len(videos)
            log(f"é™åˆ¶å¤„ç†å‰ {limit} ä¸ªè§†é¢‘", "INFO")
        
        # å¤„ç†æ¯ä¸ªè§†é¢‘
        for video in videos:
            try:
                await self.process_video(video, crawl_comments)
                await asyncio.sleep(1)  # è§†é¢‘é—´éš”
            except KeyboardInterrupt:
                log("ç”¨æˆ·ä¸­æ–­", "WARNING")
                break
            except Exception as e:
                log(f"å¤„ç†è§†é¢‘å¤±è´¥: {e}", "ERROR")
                self.stats['failed_videos'] += 1
                continue
        
        # ç»Ÿè®¡
        print()
        log("=" * 60, "INFO")
        log("çˆ¬å–å®Œæˆ", "SUCCESS")
        log(f"  è§†é¢‘æ€»æ•°: {self.stats['total_videos']}", "INFO")
        log(f"  å·²å¤„ç†: {self.stats['processed_videos']}", "INFO")
        log(f"  å¤±è´¥: {self.stats['failed_videos']}", "INFO")
        log(f"  è¯„è®ºæ€»æ•°: {self.stats['total_comments']}", "INFO")
        log(f"  äºŒçº§è¯„è®º: {self.stats['total_replies']}", "INFO")
        log(f"  è¾“å‡ºç›®å½•: {self.output_dir}", "INFO")
        log("=" * 60, "INFO")


async def main_async():
    parser = argparse.ArgumentParser(description='æŠ–éŸ³åˆé›†çˆ¬è™« - çº¯APIç‰ˆæœ¬')
    parser.add_argument('--mix-id', type=str, required=True,
                        help='åˆé›†IDï¼Œä¾‹å¦‚: 7326746646719498279')
    parser.add_argument('--output', type=str, default='output_api',
                        help='è¾“å‡ºç›®å½•ï¼Œé»˜è®¤: output_api')
    parser.add_argument('--max-comments', type=int, default=2000,
                        help='æ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°ï¼Œé»˜è®¤: 2000')
    parser.add_argument('--limit', type=int, default=0,
                        help='é™åˆ¶å¤„ç†çš„è§†é¢‘æ•°é‡ï¼Œ0è¡¨ç¤ºå…¨éƒ¨')
    parser.add_argument('--no-comments', action='store_true',
                        help='ä¸æŠ“å–è¯„è®º')
    parser.add_argument('--no-replies', action='store_true',
                        help='ä¸æŠ“å–äºŒçº§è¯„è®º')
    
    args = parser.parse_args()
    
    crawler = DouyinAPICrawler(
        output_dir=args.output,
        max_comments=args.max_comments,
        fetch_replies=not args.no_replies,
    )
    
    await crawler.crawl_mix(args.mix_id, crawl_comments=not args.no_comments, limit=args.limit)


def main():
    asyncio.run(main_async())


if __name__ == '__main__':
    main()
